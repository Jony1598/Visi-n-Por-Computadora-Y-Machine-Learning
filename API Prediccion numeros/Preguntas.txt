1. ¿Qué pasa si quitas neuronas a la capa oculta?


Disminuye la capacidad del modelo para representar patrones complejos.

Reduce el número de parámetros, por lo que el entrenamiento es más rápido.

Existe un mayor riesgo de que el modelo tenga un rendimiento inferior, especialmente si los datos contienen variaciones que requieren mayor capacidad para ser diferenciadas.

Puede ayudar a reducir el sobreajuste si el modelo original era demasiado grande para la tarea.


2. ¿Qué pasa si agregas más de 3 capas ocultas con 60 neuronas cada una?
Efectos:

Aumenta significativamente la capacidad del modelo para aprender representaciones más complejas.

Incrementa el número de parámetros, por lo tanto, el tiempo de entrenamiento también aumenta.

Puede conducir a un mayor riesgo de sobreajuste, especialmente en datasets simples como MNIST.

Puede ser más difícil de entrenar correctamente debido a problemas como el desvanecimiento del gradiente si no se aplican técnicas adecuadas (por ejemplo, normalización por lotes, funciones de activación adecuadas, etc.).


3. ¿Qué pasa con una capa oculta de 128 neuronas?

Se incrementa la capacidad del modelo para aprender patrones más complejos.

Puede mejorar el rendimiento si el modelo anterior era limitado en capacidad.

También incrementa el número de parámetros y el riesgo de sobreajuste si no se controla adecuadamente.

Puede ser beneficioso si se combina con regularización y técnicas de control del sobreajuste.

